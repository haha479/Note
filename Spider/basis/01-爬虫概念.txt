爬虫的设计思路:
1.首先确定需要爬取的网页URL地址
2. 通过HTTP/HTTPS协议来获取对应的HTML页面
3.提取HTML页面里有用的数据
	如果是需要的数据,就保存起来
	如果是页面里的其他URL，那就继续执行第二步

1.如何抓取HTML页面
	HTTP请求的处理，urllib、urllib2、requests(相当与urllib3)
	处理后的请求可以模拟浏览器发送请求，获取服务器响应的文件

2.解析服务器响应的内容
	re、xpath、Beautifulsoup4(bs4)、jsonpath、Pyquery
	使用某种描述性一样来给我们需要提取的数据定义一个匹配规则，
	符合这个规则的数据就会被匹配.

3.如何采集动态HTML,验证码的处理
	通用的动态页面采集：Selenium + PhantomJS(无界面):模拟真实浏览器加载JS,ajax等非静态页面数据

	Tesseract: 机器学习库(识别验证码)
	机器图像识别系统,可以处理简单的验证码，复杂的验证码可以通过
	手动输入/专门的打码平台

4.Scrapy:  (Scrapy,Pyspider)
	高定制性高性能(底层代码量少) (异步网络框架twisted),所以数据下载速度非常快，提供了数据存储，数据下载，提取规则等组件

5.分布式策略(采集大批量数据用)
	scrapy-redis,在scrapy的基础上添加了一套以redis数据库为核心的一套组件，让Scrapy框架支持分布式的功能,主要在redis中请求指纹去重,请求分配，数据临时存储。

6.爬虫，反爬虫，反反爬虫


通用爬虫，聚焦爬虫

通用爬虫: 爬虫程序员写的针对某种内容爬虫
	Robots协议：协议会指明通用爬虫可以爬取网页的权限
通用爬虫工作流程：爬取网页--存储数据--内容处理--提供检索/排名服务

搜索引擎排名：
	1.PageRank值：根据网站的流量（点击量/浏览量/人气）统计，流量越高，网站越值钱
	2.竞价排名：谁给钱多，谁排名就高

通用爬虫的缺点：只能提供和文本相关的内容（HTML,Word,PDF）等等，不能提供多媒体（音乐，图片），二进制文件

提供的结果千篇一律，不能针对不同背景领域的人提供不同的搜索结果

不能理解人类语义上的检索


聚焦爬虫：爬虫程序员写的针对某种内容爬虫

	面向主题爬虫，面向需求爬虫：会针对某种特定的内容去爬取信息，而且会保证信息和需求尽可能相关。

更改代理为抓包工具

















