Hive概念
	Hive本质是一个Hadoop客户端,用于将HQL(Hive SQL)转化成MapReduce程序
	1.Hive中每张表的数据存储在HDFS
	2.Hive分析数据底层的实现是MapReduce(也可配置为Spark或Tez)
	3.执行程序运行在Yarn上
	4.Hive将元数据存储在数据库中(元数据: 表名到路径的映射)

配置Hive元数据存储到mysql
	1.mysql中创建数据库metastore
	2.拷贝mysql-connector-java-...jar至hive/lib
	3.删除原有的元数据库derby产生的数据,在hive/metastore_db(初始化时指定的数据库)
	4.配置conf/hive-site.xml文件(需新建)
	5.初始化元数据库
		bin/schematool -dbType mysql -initSchema -verbose

hiveserver2
	Hive的hiveserver2服务的作用是提供jdbc/odbc接口,为用户提供远程访问Hive数据的功能.
	部署:
	1.配置hadoop中core-site.xml并分发给集群所有节点
	2.配置hive中hive-site.xml
	3.启动hiveserver2
		nohup hive --service hiveserver2 1>/dev/null 2>&1 &    #非阻塞运行

	测试:
		使用hive中自带客户端beeline测试连接
		在任一节点中使用beeline测试连接
			beeline -u jdbc:hive2://hadoop102:10000 -n self479

metastore服务
	hive的metastore服务的作用是为Hive CLI或Hiveserver2提供元数据访问接口,无需为每个客户端配置元数据库身份验证(不负责存储元数据)
	metastore默认为嵌入式,即metastore嵌入在客户端中, 需要在客户端配置jdbc4个参数.
	metastore独立服务模式
		Hive CLI或Hiveserver2通过metastore访问元数据库
	配置
		metastore服务端配置hive-site.xml文件(一台计划节点配置), 内容为jdbc连接的URL,Driver,Usernmane,Password
		启动metastore服务
			nohup hive --service metastore &
		metastore客户端配置hive-site.xml文件, 内容为指定metastore服务的地址与端口号

非交互式运行 : hive -e "sql语句"
				hive  -f  sql文件

参数配置的方式
	1.修改配置文件
		conf/hive-default.xml已经过时, 配置无效.
		建议配置写在conf/hive-site.xml中
	2.命令行参数方式
		仅本次进程启动有效
		如:hive -hiveconf mapreduce.job.reduces=10;
	3.参数声明方式
		在hive交互式客户端中输入命令(当前客户端有效)
		如获取:set mapreduce.job.reduces
		设置:set mapreduce.job.reduces
	优先级:配置文件<命令行参数<参数声明

修改log保存路径
	修改conf/hive-log4j2.properties文件中参数"property.hive.log.dir"

修改hive堆内存
	修改con/hive-env.sh中参数export HADOOP_HEAPSIZE

hive on spark
	上传spark纯净的jar包目录至hdfs(纯净的jar包是指不含hadoop依赖的spark安装包, 官网中可下载)
		hdfs dfs -put spark/jars/* /spark/jars/  (/spark/jars/在hdfs中需要事先存在)

	修改hive-site.xml文件

	hive on spark和hive on MR的区别: 
		前者是一个hive会话对应一个spark的job, 会话退出后, yarn上的资源就会释放.
		后者是一个sql对应生成一个MR的job, 即一个MR程序.
	
DDL
	创建数据库
		create database [if not exists] database_name
		[comment 注释]
		[location hdfs_path]
		[with dbproperties (property_name=property_value, ...)]
	查看数据库
		show databases;
	查看某个数据库相关信息
		describe database [extended] db_name; 
		#extended将展示更为详细的信息,包括在建库时设置的property.
	修改数据库
		alter database database_name 
		set (dbproperties/location/owner user) (property_name=../hdfs_path/user_name)
	删除数据库
		drop database [if exists] database_name [restrict|cascade]
		restrict:严格模式(默认),若数据库不为空,则删除失败
		cascade:级联模式,若数据库不为空,则将库中的表一并删除
	切换当前数据库
		use database_name

复杂数据类型
	array
		1.定义
			array<string>
		2.取值
			array[0]
		3.构造
			array(val1,val2,...);split();collect_set()
	map
		1.定义
			map<string,bigint>
		2.取值
			map[key]
		3.构造
			map(key1,value1,key2,value2,...),str_to_map(text[,delimiter1,delimiter2])
	struct
		1.定义
			struct<id:int,name:string>
		2.取值
			struct.id
		3.构造
			named_struct(name1,val1,name2,val2,...)

拉链表
	记录每条信息的生命周期, 一旦一条记录的生命周期结束, 就重新开始一条新的记录, 并把当前日期放入生效开始日期.
	如果当前信息至今有效, 则在生效结束日期中填入一个极大值.
	拉链表的意义在于能够更高效的保存维度信息的历史状态.
	每一行表达的是一个维度对象的一个状态, 该状态可能是历史的, 可能是最新的.
	拉链表适合于数据会发生变化, 但是变化频率并不高的维度.
内部表和外部表
	内部表由hive管理, 外部表由HDFS管理, 体现在删除表时, 
	内部表的元数据和hdfs路径都将被删除,
	外部表的元数据将被删除, hdfs中文件依旧存在.

	默认建表为内部表(建表时未加external修饰)


分区
	分区表指一个表被分为多个分区.
	创建分区表时, 指定分区字段, 插入数据时, 需指定分区字段的值(以分区为单位).
	在hdfs目录中, 会将数据按不同分区存储在不同的文件中.
	分区字段也可视为普通字段, 可用于查询也可用于被查询.
	分区表的意义在于优化查询, 查询时利用分区字段筛选, 则可以扫描hdfs中指定的文件.
	msck 
		msck repair table 分区表名
		修复分区, 保持元数据与hdfs目录一致
	动态分区
		动态分区是指向分区表中insert数据时, 被写往的分区不由用户指定, 而是由每行数据的最后一个字段决定.
		使用动态分区, 可只用一个insert语句	将数据写入多个分区.

窗口函数
	基于行的窗口划分
		[函数] over(partition by 分组字段 order by 排序字段 rows between 左区间 and 右区间)
		左区间可选参数
			unbounded preceding:从第一行开始划分
			[num] preceding:从指定行开始
			current row:从当前行开始
			[num] following:从当前行的往后指定行开始
		右区间可参数
			unbounded preceding:在第一行结束划分
			[num] preceding:在指定行结束
			current row:在当前行结束
			[num] following:在当前行的往后指定行结束
	基于值的窗口划分
		[函数] over(partition by 分组字段 order by 排序字段 range between 左区间 and 右区间)
		order by 后接的字段就是在划分时基于这个字段的值进行划分
		左区间可选参数
			unbounded preceding:以负无穷大作为左区间划分
			[num] preceding:以当前行划分字段值减去num,得到的值作为左区间
			current row:以当前行字段值作为左区间
			[num] following:以当前行划分字段加上num, 得到的值作为左区间
		右区间可选参数
			unbounded preceding:以正无穷大作为右区间划分
			[num] preceding:以当前行划分字段值减去num,得到的值作为右区间
			current row:以当前行字段值作为右区间
			[num] following:以当前行划分字段加上num, 得到的值作为右区间

UDTF
	常用UDTF函数(将接收的容器数据炸裂为一个表)
	explode(array<>): 返回多行一列的表，列中的数据为数组中每一个元素
	explode(map<k,v>):返回多行两列的表, 一列数据为key, 一列数据为value
	posexplode(array<>):返回多行两列的表, 一列数据为数组中每一个元素, 一列数据为数组中元素对应的索引
	inline(array<struct>):返回多行多列的表, 每列中的数据对应结构体中每个字段的值.

	Lateral View(大多数情况结合UDTF函数使用)
	lateral view首先为原始表的每行调用UDTF函数,并且将每一行生成为多行(行数由UDTF函数炸裂的数组决定),多行之间不同的是新增的字段(该字段是UDTF炸裂的数组单个元素),
	lateral view再把结果组合,产生一个支持别名表的虚拟表
	select  (虚拟表tmp中字段)
	from table lateral view explode(array) tmp as item
	-- array为待炸裂的数组字段, tmp为虚拟表别名, item为炸裂之后新产生列的别名,若炸裂的字段类型为map则别名应该有两个, 用逗号分隔.


数据倾斜
	数据倾斜是指参与计算的数据分布不均匀，即某个key或者某些key的数据量远超其他key，
	导致在shuffle阶段，大量相同key的数据被发往同一个reduce，导致该reduce所需的时间远超其他reduce，成为整个任务的瓶颈。
	解决办法
		map-side聚合
			在map端将数据进行分区预聚合，之后再将数据发往reduce端后(相同key的数据量变小了，在reduce端压力没那么大)，在reduce端聚合。
			开启map-side
				set hive.map.aggr=true;

		Skew-GroupBy优化
			Hive提供的专门解决由groupby导致的数据倾斜。
			原来是启动两个MR任务，第一个MR按照随机数分区，将数据分散发送到Reduce，完成部分聚合，第二个MR按照分组字段分区，完成最终聚合。
			开启Skew-GroupBy
				set hive.groupby.skewindata=true;

		map join
			解决在hive中join操作中大表join小表导致的数据倾斜
			hive默认使用的是common join，即在reduce端join，map join则是在map端join
			原理：整个过程分为2个task，taskA将小表的数据写入本地的文件中，之后加载到内存中。
			taskB是一个没有Reduce的MR，扫描大表的数据，在Map阶段，根据大表的每一条记录去和内存中的小表关联，直接输出结果。整个过程省去了shuffle和reduce操作。
			开启map join
				set hive.auto.convert.join=true;
			设置表的大小多大时视为小表
				set hive.mapjoin.smalltable.filesize=250000;