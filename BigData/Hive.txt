Hive概念
	Hive本质是一个Hadoop客户端,用于将HQL(Hive SQL)转化成MapReduce程序
	1.Hive中每张表的数据存储在HDFS
	2.Hive分析数据底层的实现是MapReduce(也可配置为Spark或Tez)
	3.执行程序运行在Yarn上
	4.Hive将元数据存储在数据库中(元数据: 表名到路径的映射)

配置Hive元数据存储到mysql
	1.mysql中创建数据库metastore
	2.拷贝mysql-connector-java-...jar至hive/lib
	3.删除原有的元数据库derby产生的数据,在hive/metastore_db(初始化时指定的数据库)
	4.配置conf/hive-site.xml文件(需新建)
	5.初始化元数据库
		bin/schematool -dbType mysql -initSchema -verbose

hiveserver2
	Hive的hiveserver2服务的作用是提供jdbc/odbc接口,为用户提供远程访问Hive数据的功能.
	部署:
	1.配置hadoop中core-site.xml并分发给集群所有节点
	2.配置hive中hive-site.xml
	3.启动hiveserver2
		nohup hive --service hiveserver2 1>/dev/null 2>&1 &    #非阻塞运行

	测试:
		使用hive中自带客户端beeline测试连接
		在任一节点中使用beeline测试连接
			beeline -u jdbc:hive2://hadoop102:10000 -n self479

metastore服务
	hive的metastore服务的作用是为Hive CLI或Hiveserver2提供元数据访问接口,无需为每个客户端配置元数据库身份验证(不负责存储元数据)
	metastore默认为嵌入式,即metastore嵌入在客户端中, 需要在客户端配置jdbc4个参数.
	metastore独立服务模式
		Hive CLI或Hiveserver2通过metastore访问元数据库
	配置
		metastore服务端配置hive-site.xml文件(一台计划节点配置), 内容为jdbc连接的URL,Driver,Usernmane,Password
		启动metastore服务
			nohup hive --service metastore &
		metastore客户端配置hive-site.xml文件, 内容为指定metastore服务的地址与端口号

非交互式运行 : hive -e "sql语句"
				hive  -f  sql文件

参数配置的方式
	1.修改配置文件
		conf/hive-default.xml已经过时, 配置无效.
		建议配置写在conf/hive-site.xml中
	2.命令行参数方式
		仅本次进程启动有效
		如:hive -hiveconf mapreduce.job.reduces=10;
	3.参数声明方式
		在hive交互式客户端中输入命令(当前客户端有效)
		如获取:set mapreduce.job.reduces
		设置:set mapreduce.job.reduces
	优先级:配置文件<命令行参数<参数声明

修改log保存路径
	修改conf/hive-log4j2.properties文件中参数"property.hive.log.dir"

修改hive堆内存
	修改con/hive-env.sh中参数export HADOOP_HEAPSIZE

hive on spark
	上传spark纯净的jar包目录至hdfs(纯净的jar包是指不含hadoop依赖的spark安装包, 官网中可下载)
		hdfs dfs -put spark/jars/* /spark/jars/  (/spark/jars/在hdfs中需要事先存在)

	修改hive-site.xml文件

	hive on spark和hive on MR的区别: 
		前者是一个hive会话对应一个spark的job, 会话退出后, yarn上的资源就会释放.
		后者是一个sql对应生成一个MR的job, 即一个MR程序.
	
DDL
	创建数据库
		create database [if not exists] database_name
		[comment 注释]
		[location hdfs_path]
		[with dbproperties (property_name=property_value, ...)]
	查看数据库
		show databases;
	查看某个数据库相关信息
		describe database [extended] db_name; 
		#extended将展示更为详细的信息,包括在建库时设置的property.
	修改数据库
		alter database database_name 
		set (dbproperties/location/owner user) (property_name=../hdfs_path/user_name)
	删除数据库
		drop database [if exists] database_name [restrict|cascade]
		restrict:严格模式(默认),若数据库不为空,则删除失败
		cascade:级联模式,若数据库不为空,则将库中的表一并删除
	切换当前数据库
		use database_name

复杂数据类型
	array
		1.定义
			array<string>
		2.取值
			array[0]
		3.构造
			array(val1,val2,...);split();collect_set()
	map
		1.定义
			map<string,bigint>
		2.取值
			map[key]
		3.构造
			map(key1,value1,key2,value2,...),str_to_map(text[,delimiter1,delimiter2])
	struct
		1.定义
			struct<id:int,name:string>
		2.取值
			struct.id
		3.构造
			named_struct(name1,val1,name2,val2,...)

拉链表
	记录每条信息的生命周期, 一旦一条记录的生命周期结束, 就重新开始一条新的记录, 并把当前日期放入生效开始日期.
	如果当前信息至今有效, 则在生效结束日期中填入一个极大值.
	拉链表的意义在于能够更高效的保存维度信息的历史状态.
	每一行表达的是一个维度对象的一个状态, 该状态可能是历史的, 可能是最新的.
	拉链表适合于数据会发生变化, 但是变化频率并不高的维度.
内部表和外部表
	内部表由hive管理, 外部表由HDFS管理, 体现在删除表时, 
	内部表的元数据和hdfs路径都将被删除,
	外部表的元数据将被删除, hdfs中文件依旧存在.

	默认建表为内部表(建表时未加external修饰)


分区
	创建分区表时, 指定分区字段, 插入数据时, 需指定分区字段的值(以分区为单位).
	在hdfs目录中, 会将数据按不同分区存储在不同的文件中.
	分区字段也可视为普通字段, 可用于查询也可用于被查询.
	分区表的意义在于优化查询, 查询时利用分区字段筛选, 则可以扫描hdfs中指定的文件.
	msck 
		msck repair table 分区表名
		修复分区, 保持元数据与hdfs目录一致
	动态分区
		动态分区是指向分区表中insert数据时, 被写往的分区不由用户指定, 而是由每行数据的最后一个字段决定.
		使用动态分区, 可只用一个insert语句	将数据写入多个分区.