sqoop
	sql + hadoop, 用于数据库与hadoop之间的数据传递.
	底层是将导入或导出的命令翻译成mapreduce程序实现.

测试是否可连接mysql数据库
	打印服务下的所有database名
		bin/sqoop list-databases --connect jdbc:mysql://主机名:3306/ --username 用户名 --password 密码

数据的导入导出有很多复杂的参数, 以下只列举了常用的参数.

导入数据
	导入指: 从非大数据集群(RDBMS)向大数据集群(HDFS, HIVE, HBASE)中传输数据, 使用import 关键字.

	RDBMS到HDFS

	导入表(使用import-all-tables导入所有表)
		bin/sqoop import \
		--connect jdbc:mysql://hadoop102:3306/数据库名 \
		--username 用户名 \
		--password 密码 \
		--table 表名 \
		--target-dir HDFS路径 \
		--delete-target-dir \ #若HDFS路径存在则删除再创建
		--num-mappers 1 \
		--fields-terminated-by "\t" #文件分隔符
	导入指定列
		 bin/sqoop import \
		--connect jdbc:mysql://hadoop102:3306/数据库名 \
		--username 用户名 \
		--password 密码 \
		--target-dir HDFS路径 \
		--delete-target-dir \ 
		--num-mappers 1 \
		--fields-terminated-by "\t" \
		--columns 列名1,列名2 \
		--table 表名

	RDBMS到HIVE
		 bin/sqoop import \
		--connect jdbc:mysql://hadoop102:3306/数据库名 \
		--username 用户名 \
		--password 密码 \
		--table 数据库名 \
		--num-mappers 1 \
		--hive-import \
		--fields-terminated-by "\t" \
		--hive-overwrite \
		--hive-table hive数据库.表名

导出数据
	导出指: 从大数据集群(HDFS, HIVE, HBASE)向非大数据集群(RDBMS)中传输数据, 使用export关键字.

	HIVE/HDFS 到RDBMS
		bin/sqoop export \
		--connect jdbc:mysql://hadoop102:3306/数据库名 \
		--username 用户名 \
		--password 密码 \
		--table staff \
		--num-mappers 1 \
		--export-dir /user/hive/warehouse/test.db/staff_hive \
		--input-fields-terminated-by "\t"
