spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎

配置IDEA中配置scala
	1.下载scala插件
		到IDEA插件官网下载与IDEA版本对应的scala插件到本地(一定要找对应的版本)
	2.安装插件
		file->setting->plugins,点击设置,点击install plugin from disk,选中本地的scala插件文件
	3.关联项目与scala包
		a.file->Project Structure->Global Libraries, 点击"+"->Scala SDK->Browser, 
		找到并选中本地的scala完整软件包(需提前在scala官网下载).
		b.右键项目,点击Add Framework Support,勾选scala

spark运行模式
	1.Local(本地)模式
	2.Standalone(独立)模式
	3.Yarn模式
		1.修改配置文件yarn-site.xml
		2.修改conf/spark-env.sh
		3.配置历史服务器
			修改spark-default.conf文件
			修改spark-env.sh文件
			启动历史服务
				sbin/start-history-server.sh

RDD
	RDD的数据处理方式类似于IO流, 也有装饰者设计模式, 将复杂的计算逻辑层层分开.
	RDD的数据只有在调用collect方法时, 才会真正执行业务逻辑操作, 之前的封装全部都是功能的扩展.
	RDD是不保存数据的, 但是IO可以临时保存一部分数据.
	为了提高并行计算的能力, RDD内部包含了分区的概念(将数据切分到多个task, 让多个executor执行)
	首选位置: task发送到executor节点时, 优先发送到含有计算数据的节点.

	创建RDD的4种方式:
		从集合(内存)中创建
			val rdd = sparkContext.makeRDD(集合)
		从外部存储创建(以行为单位读取文件中数据)
			val rdd = sparkContext.textFile("本地路径或HDFS路径")
		从其他RDD创建
			一个RDD运算完成, 返回一个新的RDD
		直接创建
	分区:
		1.rdd的计算一个分区内的数据是一个一个执行逻辑,
		只有前面一个数据全部的逻辑执行完毕后, 才会执行下一个数据.
		2.分区内数据的执行是有序的, 不同分区数据计算是无序的(不同分区执行是并行的).
		创建RDD时可以指定分区数.
			创建时语句的第二个参数即为分区数.
		不指定分区默认为当前环境的CPU核数(使用makeRDD方式创建).
	RDD方法(算子):
		1.转换算子: 功能的补充和封装, 旧的RDD包装成新的RDD
			map(): 接收一个函数f, 将调用者的所有元素传入函数f中逐个执行并返回结果.
			mapPartitions(): 接收一个函数f, 要求f接收一个迭代器(整个分区的数据), 返回一个迭代器,类似批处理.以分区为单位进行数据转换操作, 但是会将整个分区的数据加载到内存进行引用.
			处理完的数据不会被释放掉, 存在对象的引用.内存小数据量大的情况下, 容易出现内存溢出.
			mapPartitionsWithIndex():类似mapPartitions方法,不同的是可以获取当前分区号. f函数接收一个当前分区号的参数, 接收一个迭代器(整个分区的数据), 返回一个迭代器.
			flatMap(): 称为扁平映射.接收一个函数f,要求函数f接收一个集合中的元素,返回一个迭代器.不同于map的一对一,flatmap是一对多.
			glom(): 将同一个分区的数据直接转换为相同类型的内存数组进行处理, 分区不变,指rdd转换的过程中分区保持一致性.
			groupBy(): 将数据源中的每一个数据进行分组判断, 数据源逐个通过函数f计算后返回的值相同的数据被分在同一个组.相同的值的数据会放置在返回的RDD的tuple中作为key.不同分区的数据通过分组后可能会进入到一个分区.
			filter(): 数据源逐个通过f函数计算返回一个boolean.将数据根据指定的规则进行筛选过滤, 符合规则的数据保留,否则丢弃.
			sample(): 从数据集中抽取数据.接收三个参数.
						1:抽取数据后是否将数据放回.
						2:如果是抽取不放回的情况下:表示数据源中每条数据被抽取的概率
						如果是抽取放回的情况下:表示数据源中每条数据被抽取的可能次数
						3:抽取数据时随机算法的种子.
			distinct(): 去重
			coalesce(): 根据数据量缩减分区,用于大数据集过滤后,提高小数据集的执行效率.该算子默认不会打乱分区数据(shuffle=false,可能导致分区数据不均衡),可以设置为true, 该算子也可用于扩大分区, 但shuffle必须设置为true(否则没有意义, 同一分区的数据通过计算后还是会进入到一个分区,无法扩大), 扩大分区一般使用repartition算子(底层就是调用coalesce()).
			sortBy(): 将数据集排序, 默认为升序, 第二个参数可以设置升降.接收的函数f返回的值作为排序的依据.

			intersection(): 计算两个RDD的交集 // rdd1.intersection(rdd2)
			union(): 计算两个RDD的并集
			subtract(): 计算两个RDD的差集
			zip(): 将两个RDD中相同位置的数据组合在一个tuple中,允许两个数据源的类型不一致,但分区数量必须一致,且要求分区中数据数量一致.

			mapValues(): 只对value做计算, 数据源所有的key保持不变.
			partitionBy(): 根据指定的分区规则对数据进行重分区, scala提供的分区器有有HashPartitioner等.也可以自己编写分区器.
			reduceByKey(): 将RDD中相同的key的数据进行value数据的聚合操作(两两计算), 有分组也有聚合.若相同key的数据只有一个则不会参与运算.
			reduceByKey支持分区内预聚合功能, 可以有效减少shuffle中落盘的数据量,提高shuffle性能.
			groupByKey(): 按照数据源中相同的key的数据进行分组.返回的RDD中有多个tuple,tuple的第一个元素为key,第二个就是相同key的value的集合.
			aggregateByKey(): 将数据根据不同的规则进行分区内计算和分区间计算.该算子存在函数柯里化, 有两个参数列表.
				第一个参数列表需要传递1个参数, 表示为初始值(用于与分区中第一个数据做运算), aggregateByKey计算返回的数据结果和初始值类型一致.
				第二个参数列表需要传递2个参数
					第一个参数表示分区内计算规则
					第二个参数表示分区间计算规则
			foldByKey(): aggregateByKey的简化, 机制是分区内和分区间的计算规则相同(即第二个参数列表只需传递一个参数).
			combineByKey(): 不同于aggregateByKey, 接收的第一个参数不一样,第一个参数表示将相同key的第一个数据进行结构的转换, 而后将其作为初始值参与运算，而不是自定义初始值(与计算无关的值).
			join(): 在类型为(K,V)和(K,W)的RDD上调用, 返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD.
			若两个数据源中key没有匹配上, 那么数据不会出现在结果中.
			leftOutJoin(): 类似于sql中的左外连接, 即存在主从之分.主RDD数据全部出现在结果中, 若匹配失败, 从RDD数据显示为None
			rightOutJoin(): 类似于sql中的右外连接.
			cogroup(): 类似于group+join
		2.行动算子: 触发任务的调度和作业的执行
			reduce(): 两两聚合进行计算, 返回结果值, 而不是RDD.
			collect(): 将不同分区的数据按照分区顺序采集到Driver端内存中, 形成数组.
			count(): 返回RDD中数据的个数.
			first(): 返回RDD中第一个数据.
			take(): 接受一个参数n, 获取n个数据
			takeOrdered(): 将数据排序后(默认升序), 取前n个数据.
			aggregate(): 类似于aggregateBykey, 不同的是aggregate会返回具体的结果, 且第一个参数(初始值)不仅会参与分区内计算, 还会参与分区间计算.
			fold(): aggregate的简化, 当分区内与分区间的计算一致时使用该算子
			countValue(): 用于统计数据源中各个数据出现的次数, 返回一个Map.
			countByKey(): 用于统计数据源中每种key出现的次数.
			saveAsTextFile(): 将RDD保存成text文件
			saveAsObjectFile(): 将RDD序列化成对象保存到文件
			saveAsSequenceFile(): 将RDD保存成Sequencefile文件
			foreach(): 分布式遍历数据源中的数据.
