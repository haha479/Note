spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎

配置IDEA中配置scala
	1.下载scala插件
		到IDEA插件官网下载与IDEA版本对应的scala插件到本地(一定要找对应的版本)
	2.安装插件
		file->setting->plugins,点击设置,点击install plugin from disk,选中本地的scala插件文件
	3.关联项目与scala包
		a.file->Project Structure->Global Libraries, 点击"+"->Scala SDK->Browser, 
		找到并选中本地的scala完整软件包(需提前在scala官网下载).
		b.右键项目,点击Add Framework Support,勾选scala

spark运行模式
	1.Local(本地)模式
	2.Standalone(独立)模式
	3.Yarn模式
		1.修改配置文件yarn-site.xml
		2.修改conf/spark-env.sh
		3.配置历史服务器
			修改spark-default.conf文件
			修改spark-env.sh文件
			启动历史服务
				sbin/start-history-server.sh

RDD
	RDD的数据处理方式类似于IO流, 也有装饰者设计模式, 将复杂的计算逻辑层层分开.
	RDD的数据只有在调用collect方法时, 才会真正执行业务逻辑操作, 之前的封装全部都是功能的扩展.
	RDD是不保存数据的, 但是IO可以临时保存一部分数据.
	为了提高并行计算的能力, RDD内部包含了分区的概念(将数据切分到多个task, 让多个executor执行)
	首选位置: task发送到executor节点时, 优先发送到含有计算数据的节点.

	创建RDD的4种方式:
		从集合(内存)中创建
			val rdd = sparkContext.makeRDD(集合)
		从外部存储创建(以行为单位读取文件中数据)
			val rdd = sparkContext.textFile("本地路径或HDFS路径")
		从其他RDD创建
			一个RDD运算完成, 返回一个新的RDD
		直接创建
	分区:
		1.rdd的计算一个分区内的数据是一个一个执行逻辑,
		只有前面一个数据全部的逻辑执行完毕后, 才会执行下一个数据.
		2.分区内数据的执行是有序的, 不同分区数据计算是无序的(不同分区执行是并行的).
		创建RDD时可以指定分区数.
			创建时语句的第二个参数即为分区数.
		不指定分区默认为当前环境的CPU核数(使用makeRDD方式创建).
	RDD方法(算子):
		1.转换算子: 功能的补充和封装, 旧的RDD包装成新的RDD
			map(): 接收一个函数f, 将调用者的所有元素传入函数f中逐个执行并返回结果.
			mapPartitions(): 接收一个函数f, 要求f接收一个迭代器(整个分区的数据), 返回一个迭代器,类似批处理.以分区为单位进行数据转换操作, 但是会将整个分区的数据加载到内存进行引用.
			处理完的数据不会被释放掉, 存在对象的引用.内存小数据量大的情况下, 容易出现内存溢出.
			mapPartitionsWithIndex():类似mapPartitions方法,不同的是可以获取当前分区号. f函数接收一个当前分区号的参数, 接收一个迭代器(整个分区的数据), 返回一个迭代器.
			flatMap(): 称为扁平映射.接收一个函数f,要求函数f接收一个集合中的元素,返回一个迭代器.不同于map的一对一,flatmap是一对多.
			glom(): 将同一个分区的数据直接转换为相同类型的内存数组进行处理, 分区不变,指rdd转换的过程中分区保持一致性.
		2.行动算子: 触发任务的调度和作业的执行
 

