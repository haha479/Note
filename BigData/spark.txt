spark是一种基于内存的快速、通用、可扩展的大数据分析计算引擎

配置IDEA中配置scala
	1.下载scala插件
		到IDEA插件官网下载与IDEA版本对应的scala插件到本地(一定要找对应的版本)
	2.安装插件
		file->setting->plugins,点击设置,点击install plugin from disk,选中本地的scala插件文件
	3.关联项目与scala包
		a.file->Project Structure->Global Libraries, 点击"+"->Scala SDK->Browser, 
		找到并选中本地的scala完整软件包(需提前在scala官网下载).
		b.右键项目,点击Add Framework Support,勾选scala

spark运行模式
	1.Local(本地)模式
	2.Standalone(独立)模式
	3.Yarn模式
		1.修改配置文件yarn-site.xml
		2.修改conf/spark-env.sh
		3.配置历史服务器
			修改spark-default.conf文件
			修改spark-env.sh文件
			启动历史服务
				sbin/start-history-server.sh

SparkCore

	RDD
		RDD的数据处理方式类似于IO流, 也有装饰者设计模式, 将复杂的计算逻辑层层分开.
		RDD的数据只有在调用collect方法时, 才会真正执行业务逻辑操作, 之前的封装全部都是功能的扩展.
		RDD是不保存数据的, 但是IO可以临时保存一部分数据.
		为了提高并行计算的能力, RDD内部包含了分区的概念(将数据切分到多个task, 让多个executor执行)
		首选位置: task发送到executor节点时, 优先发送到含有计算数据的节点.

		创建RDD的4种方式:
			从集合(内存)中创建
				val rdd = sparkContext.makeRDD(集合)
			从外部存储创建(以行为单位读取文件中数据)
				val rdd = sparkContext.textFile("本地路径或HDFS路径")
			从其他RDD创建
				一个RDD运算完成, 返回一个新的RDD
			直接创建
		分区:
			1.rdd的计算一个分区内的数据是一个一个执行逻辑,
			只有前面一个数据全部的逻辑执行完毕后, 才会执行下一个数据.
			2.分区内数据的执行是有序的, 不同分区数据计算是无序的(不同分区执行是并行的).
			创建RDD时可以指定分区数.
				创建时语句的第二个参数即为分区数.
				不指定分区默认为当前环境的CPU核数(使用makeRDD方式创建).
			可自定义分区规则, 创建分区类继承Partitioner类.
		RDD方法(算子):
			1.转换算子: 功能的补充和封装, 旧的RDD包装成新的RDD
				map(): 接收一个函数f, 将调用者的所有元素传入函数f中逐个执行并返回结果.
				mapPartitions(): 接收一个函数f, 要求f接收一个迭代器(整个分区的数据), 返回一个迭代器,类似批处理.以分区为单位进行数据转换操作, 但是会将整个分区的数据加载到内存进行引用.
				处理完的数据不会被释放掉, 存在对象的引用.内存小数据量大的情况下, 容易出现内存溢出.
				mapPartitionsWithIndex():类似mapPartitions方法,不同的是可以获取当前分区号. f函数接收一个当前分区号的参数, 接收一个迭代器(整个分区的数据), 返回一个迭代器.
				flatMap(): 称为扁平映射.接收一个函数f,要求函数f接收一个集合中的元素,返回一个迭代器.不同于map的一对一,flatmap是一对多.
				glom(): 将同一个分区的数据直接转换为相同类型的内存数组进行处理, 分区不变,指rdd转换的过程中分区保持一致性.
				groupBy(): 将数据源中的每一个数据进行分组判断, 数据源逐个通过函数f计算后返回的值相同的数据被分在同一个组.相同的值的数据会放置在返回的RDD的tuple中作为key.不同分区的数据通过分组后可能会进入到一个分区.
				filter(): 数据源逐个通过f函数计算返回一个boolean.将数据根据指定的规则进行筛选过滤, 符合规则的数据保留,否则丢弃.
				sample(): 从数据集中抽取数据.接收三个参数.
							1:抽取数据后是否将数据放回.
							2:如果是抽取不放回的情况下:表示数据源中每条数据被抽取的概率
							如果是抽取放回的情况下:表示数据源中每条数据被抽取的可能次数
							3:抽取数据时随机算法的种子.
				distinct(): 去重
				coalesce(): 根据数据量缩减分区,用于大数据集过滤后,提高小数据集的执行效率.该算子默认不会打乱分区数据(shuffle=false,可能导致分区数据不均衡),可以设置为true, 该算子也可用于扩大分区, 但shuffle必须设置为true(否则没有意义, 同一分区的数据通过计算后还是会进入到一个分区,无法扩大), 扩大分区一般使用repartition算子(底层就是调用coalesce()).
				sortBy(): 将数据集排序, 默认为升序, 第二个参数可以设置升降.接收的函数f返回的值作为排序的依据.

				intersection(): 计算两个RDD的交集 // rdd1.intersection(rdd2)
				union(): 计算两个RDD的并集
				subtract(): 计算两个RDD的差集
				zip(): 将两个RDD中相同位置的数据组合在一个tuple中,允许两个数据源的类型不一致,但分区数量必须一致,且要求分区中数据数量一致.

				mapValues(): 只对value做计算, 数据源所有的key保持不变.
				partitionBy(): 根据指定的分区规则对数据进行重分区, scala提供的分区器有有HashPartitioner等.也可以自己编写分区器.
				reduceByKey(): 将RDD中相同的key的数据进行value数据的聚合操作(两两计算), 有分组也有聚合.若相同key的数据只有一个则不会参与运算.
				reduceByKey支持分区内预聚合功能, 可以有效减少shuffle中落盘的数据量,提高shuffle性能.
				groupByKey(): 按照数据源中相同的key的数据进行分组.返回的RDD中有多个tuple,tuple的第一个元素为key,第二个就是相同key的value的集合.
				aggregateByKey(): 将数据根据不同的规则进行分区内计算和分区间计算.该算子存在函数柯里化, 有两个参数列表.
					第一个参数列表需要传递1个参数, 表示为初始值(用于与分区中第一个数据做运算), aggregateByKey计算返回的数据结果和初始值类型一致.
					第二个参数列表需要传递2个参数
						第一个参数表示分区内计算规则(与reduceByKey相同, 是所有相同key的value之间做计算)
						第二个参数表示分区间计算规则(与reduceByKey相同, 是所有相同key的value之间做计算)
				foldByKey(): aggregateByKey的简化, 机制是分区内和分区间的计算规则相同(即第二个参数列表只需传递一个参数).
				combineByKey(): 不同于aggregateByKey, 只有一个参数列表,接收的第一个参数不一样,第一个参数表示将相同key的第一个数据进行结构的转换, 而后将其作为初始值与之后的value参与运算，而不是自定义初始值(与计算无关的值).
				join(): 在类型为(K,V)和(K,W)的RDD上调用, 返回一个相同key对应的所有元素连接在一起的(K,(V,W))的RDD.
				若两个数据源中key没有匹配上, 那么数据不会出现在结果中.
				leftOutJoin(): 类似于sql中的左外连接, 即存在主从之分.主RDD数据全部出现在结果中, 若匹配失败, 从RDD数据显示为None
				rightOutJoin(): 类似于sql中的右外连接.
				cogroup(): 类似于group+join
			2.行动算子: 触发任务的调度和作业的执行
				reduce(): 两两聚合进行计算, 返回结果值, 而不是RDD.
				collect(): 将不同分区的数据按照分区顺序采集到Driver端内存中, 形成数组.
				count(): 返回RDD中数据的个数.
				first(): 返回RDD中第一个数据.
				take(): 接受一个参数n, 获取n个数据
				takeOrdered(): 将数据排序后(默认升序), 取前n个数据.
				aggregate(): 类似于aggregateBykey, 不同的是aggregate会返回具体的结果, 且第一个参数(初始值)不仅会参与分区内计算, 还会参与分区间计算.
				fold(): aggregate的简化, 当分区内与分区间的计算一致时使用该算子
				countValue(): 用于统计数据源中各个数据出现的次数, 返回一个Map.
				countByKey(): 用于统计数据源中每种key出现的次数.
				saveAsTextFile(): 将RDD保存成text文件
				saveAsObjectFile(): 将RDD序列化成对象保存到文件
				saveAsSequenceFile(): 将RDD保存成Sequencefile文件
				foreach(): 分布式遍历数据源中的数据.
		RDD序列化
			闭包检测
				算子以外的代码都是在Driver端执行, 算子里面的代码都是在 Executor端执行.算子内经常可能会用到算子外的数据, 这样就
				形成了闭包的效果，如果使用的算子外的数据是对象且未执行序列化，就意味着无法传值给executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测

		RDD依赖关系
			RDD进行计算时, 旧的RDD计算会生成新的RDD, 因此多个RDD之间就形成了依赖关系.
			RDD为了提供容错性, 需要将RDD间的关系保存下来, 一旦出现错误, 可以根据血缘关系将数据源重新读取进行计算.

			新的RDD的一个分区的数据依赖于旧的RDD一个分区的数据称之为OneToOne(窄)依赖.
			新的RDD的一个分区的数据依赖于旧的RDD多个分区的数据称之为Shuffle(宽)依赖.
		RDD持久化
			将RDD保存至内存或文件中称为持久化.
			如果一个RDD需要重复使用, 则需要从头再次执行来获取数据.
			持久化rdd至内存
				rdd.persist(StorageLevel.MEMORY_ONLY) //参数可选DISK_ONLY持久化至磁盘
				持久化至磁盘文件也是临时的, 作业执行完后会自动删除.
			RDD持久化不一定只是为了重用, 数据量太大或较重要时也可以使用持久化.

			检查点checkpoint
				rdd.checkpoint()
				将RDD中间结果写入磁盘, 作业执行完后不会被删除, 一般保存路径都是在HDFS中.
				与持久化一样, 对RDD进行checkpoint操作并不会马上被执行, 必须执行行动操作才能触发.
			区别:
				cache: 底层实际调用了persist, 只有一个默认的缓存级别MEMORY_ONLY.将数据临时存储在内存中进行数据重用, 会在血缘关系中添加新的依赖.
				persist: 有多种缓存级别可供选择, 可能涉及到磁盘IO, 性能较低, 数据安全,如果作业执行完毕, 临时保存的数据文件也会丢失.
				checkpoint: 将数据长久的保存在磁盘文件中进行数据重用, 涉及到磁盘IO, 性能较低, 数据安全,
				会开启额外的job来执行checkpoint操作, 并且会重建RDD血缘关系.一般情况下需要和cache联合使用(将需要执行checkpoint的RDD使用cache缓存后, 可以直接被读取, 而无需再从头计算一次).
		RDD的文件读取和保存
			RDD可用于读取或保存的文件有3种类型
				text, sequence, object
	累加器
		分布式共享只写变量(Executor之间不能读取对方的累加器),用来把Executor端变量信息聚合到Driver端. 在Driver程序中定义的变量, 在Executor端的每个Task都会得到这个变量的一份新的副本, 
		每个task更新这些副本的值后, 传回Driver端进行merge. 累加器有多种类型, 多种方法.
		少加: 转换算子中调用累加器, 若没有行动算子, 则不会执行.
		多加: 若有多个行动算子, 则会重复累加.
		Spark默认提供了简单数据聚合的累加器, 用户可自定义累加器.

	广播变量
		闭包数据都是以Task为单位发送的, 若一个Executor中包含多个Task, 每个Task中包含闭包数据, 可能导致一个Executor中含有大量重复的数据,
		因此, 广播变量(分布式共享只读变量)可以将闭包数据存储在Executor的内存中, Task则可以共享它, 且不能更改.

SparkSQL
	DataFrame
		DataFrame是一种以RDD为基础的分布式数据集, 类似于传统数据库中的二维表格.
		DataFrame与RDD的主要区别在于, 前者带有schema元信息, 即DataFrame所表示的二维表数据集的每一列都带有名称和类型
	DataSet
		DataSet是具有强类型的数据集合, 需要提供对应的类型信息

	RDD, DataFrame, DataSet三者之间的转换, 从左向右, 包装数据.从右向左, 拆开类型, 拿出数据.
		转换时需要引入隐式转换规则: import spark.implicits._ (这里的spark是指创建的SparkSession对象)

	数据的加载和保存
		SparkSQL默认读取和保存的文件格式为parquet(列式存储格式)

		spark.read.load("path")默认加载的为parquet格式文件(否则报错).
		spark.read.format([json,jdbc,csv..]).load("path").option()加载指定格式的文件,
		option:指定额外的选项,例如读取jdbc数据时就需要指定用户名密码等.

		df.write.save("path")将数据保存为parquet格式的文件
		df.write.format([json,jdbc,csv..]).save("path").option()保存为指定格式的文件

		对于DataFrame创建一个临时表(作用域为session)或全局表(作用域为应用), 之后可以使用sql语句查询表数据.
			df.createOrReplaceTempView("表名") // 临时
			df.createGlobalTempView("表名") // 全局

		连接mysql读取或写入数据
			多个option指定jdbc参数
			读取mysql中数据
			spark.read.format("jdbc")
			.option("url", "jdbc:mysql://ip:3306/数据库名")
			.option("driver", "con.mysql.jdbc.Driver")
			.option("user", "用户")
			.option("password", "密码")
			.option("dbtable", "表名")
			.load()

			往mysql中写数据
			df.write.format("jdbc")
			.option("url", "jdbc:mysql://ip:3306/数据库名")
			.option("driver", "con.mysql.jdbc.Driver")
			.option("user", "用户")
			.option("password", "密码")
			.option("dbtable", "表名")
			.mode(Savemode.Append)
			.save()

		连接hive
			将hive-site.xml拷贝到conf/下
			将mysql驱动ysql-connector-java-...jar拷贝到jars/下
			若访问不到hdfs, 则需要把core-site.xml和hdfs-site.xml拷贝到conf/下

			IDEA开发环境下使用SparkSQL连接hive
				1.拷贝hive-site.xml,core-site,hdfs-site.xml文件到target/classes/下
				2.启用hive的支持(SparkSession.enableHiveSupport())
				3.pom中增加对应的依赖关系(包含MySQL)


SparkStreaming
	SparkStreaming用于流式数据的处理.
	SparkStreaming使用离散化流(discretized stream)作为抽象表示,叫作DStream. 
	DStream是随时间推移而收到的数据的序列, 在内部, 每个时间区间收到的数据(采集器采集到的数据)都作为RDD存在, 而DStream是由这些RDD所组成的序列(因此得名“离散化”), 并发往Driver.
	所以简单来讲, DStream就是对RDD在实时数据处理场景的一种封装。

	自定义数据源
		1. 继承Receiver, 定义泛型(采集到数据的类型), 传递参数
		2. 重写方法(onStart, onStop)

	Dstream转化
		无状态转换操作
			无状态数据操作, 只对当前的采集周期内的数据进行处理.把简单的RDD转换操作应用到每个批次上, 也就是转换Dstream中的每一个RDD.
			无状态转换操作有: map, flatMap, filter, repartition, reduceByKey, groupByKey..
			例如: reduceByKey会归约每个时间区间中的数据, 但不会归约不同区间之间的数据.
			transform: 可以将底层RDD获取到后进行操作

		有状态转换操作
			使用有状态转换操作时, 需要设定检查点路径.

			updateStateByKey: 根据key对数据的状态进行更新, 传递两个参数, 
			第一个值表示相同的key的value数据
			第二个值表示缓存区相同key的value数据

			窗口操作
				Dstream.window(窗口大小, 滑动步长)

		Dstream输出操作
			foreachRDD(func): 遍历Dstream中的每一个RDD并且执行func函数.

		关闭采集器
			//需要创建新的线程, 将关闭操作放置于线程中.而且需要在第三方程序中增加关闭状态(控制内部程序关闭)
			// stopGracefully: 优雅的关闭.计算节点不再接收新的数据, 而是将现有的数据处理完毕, 然后关闭
			ssc.stop(stopSparkContext=true, stopGracefully=true)


			恢复数据
				StreamingContext.getActiveOrCreate