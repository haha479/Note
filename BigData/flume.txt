Flume是一个高可用, 高可靠的, 分布式的海量日志采集、聚合和传输的系统.基于流式架构.

Flume主要的作用就是实时读取服务器本地磁盘的数据, 将数据写入到HDFS.

Flume主要组件为agent.
	agent包括3个组件: source, channel, sink
		source: 数据源, 可以处理各种类型, 各种格式的日志数据
		channel: 位于Source和Sink之间用于存放Event的缓冲区
		sink: 用于拉取channel中的数据写入到目的地(如HDFS文件系统)

安装flume
	解压文件
	配置conf/log4j.properties中日志文件路径
	删除lib/guava-11.0.2.jar
	依据需求修改conf/flume-env.sh.template文件
		export JAVA_OPTS="flume可以使用的最小最大堆内存"
		
启动flume(将flume运行日志展示到console)
	bin/flume-ng agent -n (agent名) -c conf/ -f (agent配置文件) -Dflume.root.logger=INFO,console

agent中三组件的类型(type)
	source
		taildir: 监控文件, 支持断点续传, 多目录
		avro: 用于flume之间互相传输, 一般配合sink中avro使用
		nc: 监控网络端口
		exec: 监控文件, 不支持断点续传
		spooling: 监控文件夹, 传输在文件夹中新创建的文件, 后续修改文件内容不做操作, 支持断点续传
		kafka: 

	channel
		file: 基于磁盘, 适用于安全需求高的数据
		memory: 基于内存
		kafka: 数据存储于kafka集群, 可选三种模式(不同数据链路)
			1. flume的source和sink
			2. flume的source和拦截器，没有sink
			3. flume的sink, 没有source
	sink
		avro
		hdfs
		kafka

flume的配置文件中可以使用%{key}的方式调用event中header中的value.
官网中HDFS Sink的原话: %{host} : 	Substitute value of event header named “host”. Arbitrary header names are supported